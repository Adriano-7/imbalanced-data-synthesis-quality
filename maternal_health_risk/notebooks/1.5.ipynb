{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf4cc9c",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Initial Setup\n",
    "\n",
    "This notebook prepares the Maternal Health Risk dataset for our experiments.\n",
    "This updated version will:\n",
    "1.  Load and clean the raw data.\n",
    "2.  Create a single, hold-out test set.\n",
    "3.  Generate multiple training datasets with varying imbalance ratios (IR) via undersampling.\n",
    "4.  For each IR, create a size-matched control dataset with the original class ratio.\n",
    "5.  Preprocess and save all generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999e9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and loading the raw dataset...\n",
      "Raw dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path(\"../data/raw/maternal_health_risk.csv\")\n",
    "PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "TARGET_FEATURE = 'RiskLevel'\n",
    "NUMERICAL_FEATURES = ['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'BodyTemp', 'HeartRate']\n",
    "RANDOM_STATE = 42\n",
    "IMBALANCE_RATIOS = [1, 2, 3, 4, 5, 10] # Majority:Minority ratios to generate\n",
    "\n",
    "RAW_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Fetching and loading the raw dataset...\")\n",
    "maternal_health_risk = fetch_ucirepo(id=863)\n",
    "X_features = maternal_health_risk.data.features\n",
    "y_target = maternal_health_risk.data.targets\n",
    "df_raw = pd.concat([X_features, y_target], axis=1)\n",
    "df_raw.to_csv(RAW_PATH, index=False)\n",
    "print(\"Raw dataset loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd765cd",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "Applying the cleaning steps identified in the EDA: removing biased duplicates and the physiological outlier (HeartRate=7).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ce3c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned. Shape after cleaning: (451, 7)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_raw.drop_duplicates().copy()\n",
    "df_cleaned = df_cleaned[df_cleaned['HeartRate'] != 7].reset_index(drop=True)\n",
    "print(f\"Data cleaned. Shape after cleaning: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd0607",
   "metadata": {},
   "source": [
    "# 3. Target Binarization\n",
    "\n",
    "To simplify the initial experiments, we convert the multi-class target into a binary one.\n",
    "- **Class 0 (Majority):** 'low risk'\n",
    "- **Class 1 (Minority):** 'mid risk' and 'high risk' are combined into a single 'at-risk' class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4072508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable distribution after binarization:\n",
      "RiskLevel\n",
      "low risk    233\n",
      "at-risk     218\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_cleaned[TARGET_FEATURE] = df_cleaned[TARGET_FEATURE].replace({'mid risk': 'at-risk', 'high risk': 'at-risk'})\n",
    "print(\"\\nTarget variable distribution after binarization:\")\n",
    "print(df_cleaned[TARGET_FEATURE].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56964c6b",
   "metadata": {},
   "source": [
    "# 4. Create a Hold-Out Test Set\n",
    "\n",
    "This is a one-time split. This test set will remain untouched and will be used for the final evaluation of all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1061442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full training set shape: (360, 7)\n",
      "Hold-out test set shape: (91, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df_cleaned.drop(TARGET_FEATURE, axis=1)\n",
    "y = df_cleaned[[TARGET_FEATURE]]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "train_full_df = pd.concat([X_train_full, y_train_full], axis=1)\n",
    "\n",
    "print(f\"\\nFull training set shape: {train_full_df.shape}\")\n",
    "print(f\"Hold-out test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa88e0",
   "metadata": {},
   "source": [
    "# 5. Generate Imbalanced and Control Datasets\n",
    "\n",
    "We now loop through the desired imbalance ratios to generate our experimental training sets. For each ratio, we create two versions:\n",
    "1.  **Imbalanced Set**: Created by undersampling the majority class.\n",
    "2.  **Control Set**: A stratified sample from the original training set that has the *same total number of instances* as the imbalanced set but preserves the *original class ratio*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8451478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set composition: 186 majority samples, 174 minority samples.\n",
      "\n",
      "Processing Imbalance Ratio (IR) = 1:1\n",
      "  - Imbalanced set created: 348 samples (174 majority, 174 minority)\n",
      "  - Control set created:      348 samples (original class ratio)\n",
      "\n",
      "Processing Imbalance Ratio (IR) = 2:1\n",
      "  - WARNING: Cannot create 2:1 ratio. Not enough majority samples.\n",
      "  - Using all available 186 majority samples instead.\n",
      "  - Imbalanced set created: 360 samples (186 majority, 174 minority)\n",
      "  - Control set created:      360 samples (original class ratio)\n",
      "\n",
      "Stopping further processing as maximum possible imbalance has been reached.\n"
     ]
    }
   ],
   "source": [
    "minority_df = train_full_df[train_full_df[TARGET_FEATURE] == 'at-risk']\n",
    "majority_df = train_full_df[train_full_df[TARGET_FEATURE] == 'low risk']\n",
    "n_minority = len(minority_df)\n",
    "n_majority_available = len(majority_df)\n",
    "\n",
    "print(f\"Full training set composition: {n_majority_available} majority samples, {n_minority} minority samples.\")\n",
    "\n",
    "generated_datasets = {}\n",
    "\n",
    "for ir in IMBALANCE_RATIOS:\n",
    "    print(f\"\\nProcessing Imbalance Ratio (IR) = {ir}:1\")\n",
    "    \n",
    "    n_majority_imbalanced = int(n_minority * ir)\n",
    "    \n",
    "    if n_majority_imbalanced > n_majority_available:\n",
    "        print(f\"  - WARNING: Cannot create {ir}:1 ratio. Not enough majority samples.\")\n",
    "        print(f\"  - Using all available {n_majority_available} majority samples instead.\")\n",
    "        n_majority_imbalanced = n_majority_available\n",
    "\n",
    "    majority_undersampled = resample(\n",
    "        majority_df,\n",
    "        replace=False,\n",
    "        n_samples=n_majority_imbalanced,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    imbalanced_df = pd.concat([majority_undersampled, minority_df])\n",
    "    total_size = len(imbalanced_df)\n",
    "    generated_datasets[f'imbalanced_ir_{ir}'] = imbalanced_df\n",
    "    \n",
    "    print(f\"  - Imbalanced set created: {total_size} samples ({len(majority_undersampled)} majority, {n_minority} minority)\")\n",
    "\n",
    "    if total_size == len(train_full_df):\n",
    "        control_df = train_full_df.copy()\n",
    "    else:\n",
    "        control_df, _ = train_test_split(\n",
    "            train_full_df,\n",
    "            train_size=total_size,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=train_full_df[TARGET_FEATURE]\n",
    "        )\n",
    "    \n",
    "    generated_datasets[f'control_ir_{ir}'] = control_df\n",
    "    print(f\"  - Control set created:      {len(control_df)} samples (original class ratio)\")\n",
    "\n",
    "    if n_majority_imbalanced == n_majority_available and ir != 1:\n",
    "        print(\"\\nStopping further processing as maximum possible imbalance has been reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367bb3a",
   "metadata": {},
   "source": [
    "# 6. Preprocessing and Saving All Datasets\n",
    "\n",
    "We fit the scaler ONCE on the full training data to learn the scaling parameters. Then, we transform all generated training sets and the hold-out test set using this single, consistent scaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99f3ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/processed/train_imbalanced_ir_1.csv\n",
      "Saved: ../data/processed/train_control_ir_1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/processed/train_imbalanced_ir_2.csv\n",
      "Saved: ../data/processed/train_control_ir_2.csv\n",
      "Saved: ../data/processed/test.csv\n",
      "\n",
      "Preprocessing complete. Datasets are ready for experiments.\n"
     ]
    }
   ],
   "source": [
    "target_encoder = OrdinalEncoder(categories=[['low risk', 'at-risk']])\n",
    "target_encoder.fit(y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full[NUMERICAL_FEATURES])\n",
    "\n",
    "for name, df in generated_datasets.items():\n",
    "    X_temp = df.drop(columns=[TARGET_FEATURE])\n",
    "    y_temp = df[[TARGET_FEATURE]]\n",
    "\n",
    "    X_processed = scaler.transform(X_temp[NUMERICAL_FEATURES])\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=NUMERICAL_FEATURES)\n",
    "    \n",
    "    y_processed = target_encoder.transform(y_temp)\n",
    "    \n",
    "    final_df = X_processed_df\n",
    "    final_df[TARGET_FEATURE] = y_processed\n",
    "    save_path = PROCESSED_PATH / f\"train_{name}.csv\"\n",
    "    final_df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "X_test_processed = scaler.transform(X_test[NUMERICAL_FEATURES])\n",
    "y_test_processed = target_encoder.transform(y_test)\n",
    "\n",
    "test_df = pd.DataFrame(X_test_processed, columns=NUMERICAL_FEATURES)\n",
    "test_df[TARGET_FEATURE] = y_test_processed\n",
    "test_df.to_csv(PROCESSED_PATH / \"test.csv\", index=False)\n",
    "print(f\"Saved: {PROCESSED_PATH / 'test.csv'}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete. Datasets are ready for experiments.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
